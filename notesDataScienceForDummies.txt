Chapter 1: (Introduction) 
    - Data science is a relatively new field first termed by William S. Cleveland in 2001
    
    - Competencies a data scientist could learn and excel :
        - Data Capture = understanding how to structure, relate, and model raw data into tangible concepts
        - Analysis = analysing the data after capture with statistical tools to help draw conclusions
        - Presentation = present the data in a way so that it is readable and visualize them to the masses and tell a specific story so that the impact of the data isn't lost
    
    - The act of moving data to be analysed is a specialty called "Extract, Transformation, and Loading" (ETL).
    - The ETL specialist uses programming languages such as Python to extract data from different sources because corporations tend to not keep data in one easily accessed location
    - In a large scale, real world operation the tools you might use are Informatica, MS SSIS, or Teradata
    - Always be objective and kee p an open mind when analysis data, having preconceived ideas of what the data contains will potentially lead to seeing patterns in the dark
    - For an applicationn to learn, it has to be able to manipulate large databses and discover new patterns within them
    - Applications must also be able to create new data base on the old data and making an educated guess or prediction
    
    - The types of programming methods supported by Python : 
        - Functional = Treats every sentence as a mathematical equation and avoids forms of state or mutable data, usually used for recursion or lambda calculus
        - Imperative = Computations directly change program's state, usually used for manipulating data structures and produces elegant, but simple code
        - Object Oriented =  Data fields treated as objects and manipulated through prescribed methods, though python doesn't fully support this because it doesn't have data hiding, it still supports encapsulation and polymosrphism
        - Procedural = Treats tasks step by step where common tasks are places in functions and called as needed

Chapter 2: (Python's Capabilities)
    - Python was made by Guido Van Rossum in 1989 as a replacement for the ABC language
    - The core concept of python was to create it as fast as possible, yet flexible, runs on any platforms, and provide significant potential for extensions
    
    - Reasons why python is preferred in data science : 
        - Library Support = python offers a large number of libraries that helps to ease the process of data science
        - Parallel Processing = python can process tasks using separate threads so that worlkload can be broken down to provide a faster process
        - Data Acess = python can access SQL databases which is useful for storing and organizing huge amounts of processed data
        - Data Display = python offers a wide array of libraries that can be used to diplay data like Quixote or Pandas
    
    - Phases of prototyping when analysing data : 
        - Building a Data Pipeline = to work with a data you must create a way for the program and data to interact
        - Performing the required Shaping = The shape of the data like it's data type is important in analysis because it can effects the way it compares with other data, and how it interacts with a particular algorithm
        - Analysing the Data = When analysing data it is best to use several algorithm to see which one does the best job and for the particular condition
        - Presenting the Result = After analysing the data you have to present it in an easily and visually, usually using MATLAB which can present a data graphically in different ways for your particular needs
    
    - Factors in the Speed of Execution :
        - Dataset Size = data science relies on big datasets to determine the right conclusions, and underestimating the effect of the size of a dataset is a mistake in real life application because it can effect on the speed of it's execution
        - Loading Techniques = the method you use to load data for analysis is critical and should always opt for the fastest means available
        - Coding Style = the particular coding style can effect the effectiveness of a given task, you just have to adapt to your particular condition
        - Machine Capability = you need a capable system to run data science because they are particularly heavy in both processor and disk
        - Analysis Algorithm = the algorithm you use determines the kind of results you obtain and speed
    
    - Scikit-learn datasets appear within bunches which is a kind of data structure
    - In every dataset there is keys that can be used to identify certain values
    - keys() function is used to diplay a list of keys that can be used to access data within the dateset
    - If you want to access the data you want you can just type the key after the variable you have set <variable>.<key>
   
    - Acessing Scientific Tools Using SciPy : 
        - NumPy = a library that provides themeans of performing different mathematical tasks like n-dimensional arrays, linear algebra, random-number generator, etc etc
        - Pandas = Pandas provides a fast and efficient way to make a data structure on the dataset that has been analysed that's called a "dataframe"
        - Scikit-Learn = one of a number of SciKit libraries that builds on the capabilities of numpy and scipy to allow domain specific tasks
    - Keras is an API that is used to train deep learning models
    - To use Keras you need an implementation of Keras to use it which is where Tensorflow comes into play
    
    - Deep Learning with Keras and Tensorflow :
        - Concistent Interface = Keras interface is optimized for common use cases
        - Lego Approach = Using black-box approach to make it easy to configure building blocks together
        - Extendable = Easily adds custom building blocks to express new ideas for research
        - Parallel Processing = Keras can run on both CPUs and GPUs
        - Direct Python Support = you don't need to do anything special to make the TensorFlow implementation of Keras work with python
    
    - The matploblib is used to plot data and gives a MATLAB like interface for creating data presentations of the analysis
    - NetworkX provides a way to study the relationship between complex data in a networked system and outputs a result in a way humans can understand
    - Beautiful soup is used to parse HTML or XML data in a matter python understands and allows you to work with tree-based data

    - Technologies Used in Data Science : 
        - Numpy
        - Pandas
        - SciKit-Learn
        - Keras
        - TensorFlow
        - Matploblib
        - Pandas
        - NetworkX
        - Beautiful Soup

Chapter 3: (Installing python and anaconda)
    - Chapter 3 is just instructions to download Python, anaconda, etc etc

Chapter 4: (Google collab)
    - DO WHEN YOU'RE AT HOME 

Chapter 5: (Understanding the tools)
    - Five commands for enchanced help :
        - ? = using jupyter to perform useful work
        - object? = lists out facts about the packages, objects, and methods in python used to interact with data
        - object?? = obtaining short information about the given packages, objects, and methods
        - %quickref = obtaining information about magic function in jupyter
        - help() = learn about python

        - help() and ? aren't the same, the former is for python while the latter is for IPython features

    - help() parameteres and what they do:
        - modules = compiles a list of the currently loaded modules
        - kewords = presents a list of python keywords
        - symbols = shows a list of symbols that have a special meaning in python like * for multiplications and << for left shift
        - topics = displays a list of general python topics, the topics appears in uppercase!
    
    - Working with magic function:
        - most magic functions starts with a single (%) or double (%%)
        - those with a single % works at the command-line level while those with double %% works at the cell level

    - Using IPython object help:
        - %pdoc = displays the dosctring for the object
        - %pdef = shows how to call the object
        - %source = displays the source code for the object
        - %file  = outputs the name of the file that contains the source code
        - %pinfo = displays detailed information about the object
        - %pinfo2 = displays extra detailed information about the object
    
    - Modes of typing in Jupyter: 
        - code = write standard python code
        - markdown = write standard HTML used to separate and add content within the file
        - raw NBconvert = include information that shouldn't be modified by the notebook converter (NBConvert) used to include special content like Lamport TeX which isn't tied for a particular editor but a means of encoding scientific documents
        - heading = used to be used for adding headings, it's no longer supported and now use #, ##, ###, descending in importance with the markdown model
    
    - Multimedia and Graphic integration in jupyter:
        - a lot of functionality required to perform multimedia tasks appears within the Ipython.display or Jupyter.display library
        - import the Image method from the IPython.display library
        
        - Example code:
            from IPythono.display import Image
            embed = image('URL' + 'url of the image')
            embed

        - when working with embedded images on a regular basis, you might want to set the form which they are embedded in
            from IPython.display import set_matploblib_formats
            set_matploblib_formats('pdf', 'svg')

        - if you expect an image to change overtime you might want to create a link to it instead of embedding it
            softLinked = Image(url='URL') instead of Embed

Chapter 6: (Finally working with real data!!)
    - the Scikit-learn library includes a number of toy datasets meant to be studied on a small scale and experiment with
    - Storing data in local computer memory represents the fastest and most reliable means to access it
    - Data scientists call the columns in a database features or variable, rows cases, and each row represents a collection of variables you can analyze
    - 